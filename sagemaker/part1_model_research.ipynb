{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS for CRISP-DM Phases 3-5: Data Prepartion, Modeling, and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are inside a Jupyter Notebook, we assume that most of you are within familiar territory. As such, this tutorial will not go into detail about these phases. Rather, we'll quickly breeze through these three phases with a focus on productionalizing this code into Sagemaker. In the next steps, we'll provide more detail on how to deploy real-time models using Sagemaker's SDK.\n",
    "\n",
    "Because this tutorial is focused on Sagemaker rather than the Data Science, we'll use a common dataset, MNIST, and train an image classifier using MXNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standard CRISP-DM Phases 3-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this tutorial will not focus on the Data Science of the modeling. As such, the following section training and evaluation code is 99% taken from https://mxnet.incubator.apache.org/tutorials/python/mnist.html\n",
    "\n",
    "If you are familiar with MXNet and the standard training and evaluation code, feel free to jump ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_name = 'jakechenawspublic'\n",
    "key_name = 'sample_data/mnist/train/mnist_train.csv'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket_name, key_name, 'mnist_train.csv')\n",
    "\n",
    "mnist_train = np.loadtxt('mnist_train.csv', delimiter=',')\n",
    "X_train = mnist_train.T[1:].T.reshape(-1,1,28,28)\n",
    "y_train = mnist_train.T[:1].T.reshape(-1)\n",
    "\n",
    "batch_size = 100\n",
    "train_iter = mx.io.NDArrayIter(X_train[:-1000], y_train[:-1000], batch_size, shuffle=True)\n",
    "val_iter = mx.io.NDArrayIter(X_train[-1000:], y_train[-1000:], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.var('data')\n",
    "# first conv layer\n",
    "conv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "# second conv layer\n",
    "conv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "# first fullc layer\n",
    "flatten = mx.sym.flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "tanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n",
    "# second fullc\n",
    "fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)\n",
    "# softmax loss\n",
    "lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a trainable module on CPU 0\n",
    "lenet_model = mx.mod.Module(symbol=lenet, context=mx.cpu()) # change to mx.gpu() if using ml.p2.xlarge\n",
    "# train with the same\n",
    "lenet_model.fit(train_iter,\n",
    "                eval_data=val_iter,\n",
    "                optimizer='sgd',\n",
    "                optimizer_params={'learning_rate':0.1},\n",
    "                eval_metric='acc',\n",
    "                batch_end_callback = mx.callback.Speedometer(batch_size, 100),\n",
    "                num_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalMetric: {'accuracy': 0.98870000000000002}\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'jakechenawspublic'\n",
    "key_name = 'sample_data/mnist/test/mnist_test.csv'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket_name, key_name, 'mnist_test.csv')\n",
    "\n",
    "mnist_test = np.loadtxt('mnist_test.csv', delimiter=',')\n",
    "X_test = mnist_test.T[1:].T.reshape(-1,1,28,28)\n",
    "y_test = mnist_test.T[:1].T.reshape(-1)\n",
    "\n",
    "test_iter = mx.io.NDArrayIter(X_test, y_test, batch_size)\n",
    "\n",
    "# predict accuracy for lenet\n",
    "acc = mx.metric.Accuracy()\n",
    "lenet_model.score(test_iter, acc)\n",
    "print(acc)\n",
    "assert acc.get()[1] > 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have probabily noticed by now, this step can take awhile since it's being trained locally on the instance currently used to host Jupyter.\n",
    "\n",
    "Instead, since we know that this code works, let's go to the next step and refactor the above code into the Sagemaker SDK. This allows us to use Sagemaker's distributed training capabilities to drastically speed up training time.\n",
    "\n",
    "In the [instructions](./part0_instructions.md), please move on to 2. Model Development for SageMaker."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
