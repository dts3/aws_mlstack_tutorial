{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS for CRISP-DM Phases 3-5: Data Prepartion, Modeling, and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are inside a Jupyter Notebook, we assume that most of you are within familiar territory. As such, this tutorial will not go into detail about these phases. Rather, we'll quickly breeze through these three phases with a focus on productionalizing this code into Sagemaker. In the next steps, we'll provide more detail on how to deploy real-time models using Sagemaker's SDK.\n",
    "\n",
    "Because this tutorial is focused on Sagemaker rather than the Data Science, we'll use a common dataset, MNIST, and train an image classifier using MXNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standard CRISP-DM Phases 3-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this tutorial will not focus on the Data Science of the modeling. As such, the following section training and evaluation code is mostly taken from https://mxnet.incubator.apache.org/tutorials/python/mnist.html\n",
    "\n",
    "If you are familiar with MXNet and the standard training and evaluation code, feel free to jump ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup, Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import boto3\n",
    "\n",
    "# download train files from s3 \"directory\"\n",
    "s3_bucket = 'jakechenawspublic'\n",
    "s3_prefix = 'sample_data/mnist/train'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)\n",
    "\n",
    "s3_keys = [r['Key'] for r in response['Contents']]\n",
    "for s3_key in s3_keys:\n",
    "    fname = key_name.replace(s3_prefix, '').lstrip('/') # create local file name by removing prefix from key name\n",
    "    s3.download_file(s3_bucket, s3_key, fname)          # download train file\n",
    "\n",
    "# load downloaded files (in this case, file) into np.array\n",
    "fnames = glob('*train.csv')\n",
    "arrays = np.array([np.loadtxt(f, delimiter=',') for f in fnames])\n",
    "\n",
    "# join files into one array with shape [records, 785]\n",
    "# 785 because each record has 28x28=784 pixels and 1 label\n",
    "mnist_train = arrays.reshape(-1, 785)\n",
    "\n",
    "# split record into image data and label\n",
    "X_train = mnist_train.T[1:].T.reshape(-1,1,28,28)\n",
    "y_train = mnist_train.T[:1].T.reshape(-1)\n",
    "\n",
    "# wrap mxnet iterator around records\n",
    "batch_size = 100\n",
    "train_iter = mx.io.NDArrayIter(X_train[:-1000], y_train[:-1000], batch_size, shuffle=True)\n",
    "val_iter = mx.io.NDArrayIter(X_train[-1000:], y_train[-1000:], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken straight from https://mxnet.incubator.apache.org/tutorials/python/mnist.html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define network\n",
    "data = mx.sym.var('data')\n",
    "# first conv layer\n",
    "conv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)\n",
    "tanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\n",
    "pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "# second conv layer\n",
    "conv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)\n",
    "tanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\n",
    "pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "# first fullc layer\n",
    "flatten = mx.sym.flatten(data=pool2)\n",
    "fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "tanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n",
    "# second fullc\n",
    "fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)\n",
    "# softmax loss\n",
    "lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a trainable module\n",
    "lenet_model = mx.mod.Module(symbol=lenet, context=mx.cpu()) # change to mx.gpu() if using ml.p2.xlarge\n",
    "# train with the same\n",
    "lenet_model.fit(train_iter,\n",
    "                eval_data=val_iter,\n",
    "                optimizer='sgd',\n",
    "                optimizer_params={'learning_rate':0.1},\n",
    "                eval_metric='acc',\n",
    "                batch_end_callback = mx.callback.Speedometer(batch_size, 100),\n",
    "                num_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalMetric: {'accuracy': 0.98870000000000002}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "\n",
    "# download test files from s3 \"directory\"\n",
    "s3_bucket = 'jakechenawspublic'\n",
    "s3_prefix = 'sample_data/mnist/test'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)\n",
    "\n",
    "s3_keys = [r['Key'] for r in response['Contents']]\n",
    "for key_name in s3_keys:\n",
    "    fname = key_name.replace(s3_prefix, '').lstrip('/') # create local file name by removing prefix from key name\n",
    "    s3.download_file(bucket_name, key_name, fname)      # download test file\n",
    "\n",
    "fnames = glob('*_test.csv')\n",
    "arrays = np.array([np.loadtxt(f, delimiter=',') for f in fnames])\n",
    "\n",
    "mnist_test = arrays.reshape(-1, 785)\n",
    "X_test = mnist_test.T[1:].T.reshape(-1,1,28,28)\n",
    "y_test = mnist_test.T[:1].T.reshape(-1)\n",
    "\n",
    "test_iter = mx.io.NDArrayIter(X_test, y_test, batch_size)\n",
    "\n",
    "# predict function for lenet\n",
    "prob = lenet_model.predict(test_iter)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have probabily noticed by now, this step can take awhile since it's being trained locally on the instance currently used to host Jupyter.\n",
    "\n",
    "Instead, since we know that this code works, let's go to the next step and refactor the above code into the Sagemaker SDK. This allows us to use Sagemaker's distributed training capabilities to drastically speed up training time.\n",
    "\n",
    "In the [instructions](./part0_instructions.md), please move on to 2. Model Development for SageMaker."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
